---
layout: post
title:  "Analyzing robust alternatives to the least squares problem"
date:   2018-03-26 12:31:34 +0200
categories: regression least-squares least-trimmed-set least-median robust
published: false
---

The standard least squares estimator is known not to deal very well with outliers. An illustration of this problem is shwon below. The dataset is generated by 

\\[y\_i = ax\_i + b + e\_i \quad \forall i \in [1, 50]\\]

with \\(a = 0.5, b = 1\\) and \\(e\_i \sim \mathcal{N}(0, 1)\\). To generate the outliers, values for \\(a, b\\) are replaced by \\(-0.25\\) and \\(40\\) with probability \\(0.2\\). The resulting dataset is shown in Figure 1.1. Examining the plot visually, it is obvious which points are generated by which mechanism. This can become difficult for more complex generation mechanism, or for higher dimensional data. The two models are shown together with the data in Figure 1.2.

Plotting a histogram of the residual errors, it is clear that one part of the errors comes from the Gaussian noise that is added to the model, while another part is produced by a different mechanism.

Applying the least squares estimator to the dataset results in the fit shown in Figure 2. The resulting fit is not what is expected, and while it minimizes the residual error, it does not e.g. minimize the median residual error. When plotting the least squares astimator on the data without the outliers, the resulting parameters fit nicely to the data as shown in Figure 2.2. Both resulting estimated parameters are shown in table [..].

Showing a similar histogram as above, it becomes clear that while the outliers are still visible, the fit produced by the least squares algorithm is not optimal.

### Alternatives to least squares
Given the information above, it becomes clear that an alternative to the Least Squares algorithm is needed. Lucklily, there is existing literature on robust regression, and some of the proposed algorithms are the following:

* Least absolute deviation: This is an obvious choice coming from the \\(L\_2\\) norm. Using the absolute value penalizes larger deviations not as much as the squared residual does. Small deviations are penalized proportinally bigger than in the other norm. A problem with the \\(L\_1\\) norm is the fact the the derivative is not continous at 0, which results in an unstable estimator.
* Combination of \\(L\_1\\) and \\(L\_2\\) norms. The huber penalty \\[L\_{huber}(u) = \left\\{\begin{array}{ll}0.5 u^2 &\lvert u \rvert \leq M \\\\ M(\lvert u\rvert - 0.5 M) & \text{else}\end{array}\right.\\] function is such a combination of the two norms. It allows for small deviations from 0, while not penalizing large deviations as harshly. In addition, the huber penalty function results in a continous first derivative.
* Least trimmed squares. The least trimmed squares function is a version of the least squares estimator, that minimizes the residuals on a subset of the original dataset.
* Least median squares. In the LMS estimator, instead of minimizing the sum of all residuals, the median of the residuals is minimized.
* Other \\(L\_p\\) norms can be used, i.e. \\(p=0.5\\).
* Sum of norms regularization
* Saturated squared residuals
